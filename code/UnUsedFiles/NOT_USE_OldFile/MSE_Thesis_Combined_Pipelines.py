#!/usr/bin/env python3
"""
MINISTRY OF EDUCATION AND TRAINING
FPT UNIVERSITY

Enhancing the User Search Experience on e-commerce Platforms Using the Deep Learning-Based Approach

Combined Pipelines: Vietnamese NL2SQL System
Google Colab Notebook for Both Pipeline 1 & 2 Implementation

Version: 1.0
Created: 2025-01-07 14:32:15 UTC
Last Modified: 2025-01-07 14:32:15 UTC

Version History:
- v1.0 (2025-01-07): Initial release with enhanced logging, removed fallback logic, version control

Author: MSE14 Duong Dinh Tho
Thesis: Master of Software Engineering
¬© Copyright by MSE 14 Duong Dinh Tho, 2025

Architecture:
- Pipeline 1: Vietnamese Query ‚Üí PhoBERT-SQL ‚Üí SQL Query
- Pipeline 2: Vietnamese Query ‚Üí PhoBERT (Vi‚ÜíEn) ‚Üí SQLCoder (En‚ÜíSQL) ‚Üí SQL Query

Usage:
1. Run this in Google Colab with GPU enabled
2. Copy the single ngrok URL for integration with local system
3. Use endpoints /pipeline1 and /pipeline2 for different approaches

IMPORTANT: This version removes ALL fallback logic - pipelines fail cleanly when models not loaded
"""

# Cell 1 - Markdown
"""
# Combined Vietnamese NL2SQL Pipelines v1.0

**MINISTRY OF EDUCATION AND TRAINING - FPT UNIVERSITY**
**Enhancing the User Search Experience on e-commerce Platforms Using the Deep Learning-Based Approach**

**Version**: 1.0  
**Created**: 2025-01-07 14:32:15 UTC  
**Last Modified**: 2025-01-07 14:32:15 UTC

**Version History**:
- v1.0 (2025-01-07): Initial release with enhanced logging, removed fallback logic, version control

Author: MSE14 Duong Dinh Tho
Master of Software Engineering
¬© Copyright by MSE 14 Duong Dinh Tho, 2025

This notebook runs both Pipeline 1 and Pipeline 2 on Google Colab GPU with a single ngrok tunnel.

**Architectures**:
- **Pipeline 1**: Vietnamese Query ‚Üí PhoBERT-SQL ‚Üí SQL Query
- **Pipeline 2**: Vietnamese Query ‚Üí PhoBERT (Vi‚ÜíEn) ‚Üí SQLCoder (En‚ÜíSQL) ‚Üí SQL Query

**Endpoints**:
- `/pipeline1` - Direct Vietnamese to SQL
- `/pipeline2` - Vietnamese ‚Üí English ‚Üí SQL

**GPU Required**: T4/V100 for optimal performance

**IMPORTANT**: This version removes ALL fallback logic - pipelines fail cleanly when models not loaded
"""

# Cell 2 - Code: Environment Setup
import subprocess
import sys

print("Installing required packages...")

# Install packages
packages = [
    "fastapi",
    "uvicorn",
    "nest-asyncio", 
    "pyngrok",
    "transformers",
    "torch",
    "datasets",
    "sentence-transformers",
    "pandas",
    "numpy",
    "requests",
    "accelerate",
    "bitsandbytes",
    "psutil",
    "colorama"
]

for package in packages:
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
        print(f"{package}")
    except subprocess.CalledProcessError:
        print(f"Failed to install {package}")

print("Package installation completed")

import torch
import logging
import time
import re
import psutil
import gc
from datetime import datetime
import tracemalloc
import hashlib
from typing import Dict, Any, List, Optional
from colorama import Fore, Back, Style, init

# Initialize colorama for colored output
init(autoreset=True)

# Check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"üìä GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cell 2.5 - Code: Enhanced Colab Logger and Evaluation Classes
class ColabLogger:
    """Enhanced Google Colab runtime logging with colored output"""
    
    def __init__(self):
        self.request_counter = 0
        
    def log_request_start(self, vietnamese_query: str, pipeline: str):
        """Display incoming Vietnamese query with request counter"""
        self.request_counter += 1
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üöÄ REQUEST #{self.request_counter} - {pipeline.upper()}")
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.YELLOW}üìù Vietnamese Query: {Style.BRIGHT}'{vietnamese_query}'")
        print(f"{Fore.BLUE}‚è∞ Started at: {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")
        
    def log_pipeline_step(self, step_name: str, duration_ms: float, details: str = ""):
        """Show each pipeline step with timing"""
        print(f"{Fore.GREEN}  ‚úì {step_name}: {Style.BRIGHT}{duration_ms:.2f}ms{Style.RESET_ALL}")
        if details:
            print(f"{Fore.WHITE}    ‚îî‚îÄ {details}")
            
    def log_response(self, sql_query: str, english_query: str = None, 
                    execution_time_ms: float = 0, success: bool = True):
        """Display final results"""
        if success:
            print(f"\n{Fore.GREEN}‚úÖ PIPELINE SUCCESS")
            if english_query:
                print(f"{Fore.MAGENTA}üåê English Translation: '{english_query}'")
            print(f"{Fore.CYAN}üìä Generated SQL: {Style.BRIGHT}{sql_query}")
            print(f"{Fore.BLUE}‚ö° Total Execution Time: {Style.BRIGHT}{execution_time_ms:.2f}ms")
        else:
            print(f"\n{Fore.RED}‚ùå PIPELINE FAILED")
            
    def log_error(self, error_message: str, pipeline: str):
        """Show pipeline errors with clear formatting"""
        print(f"\n{Fore.RED}{'='*80}")
        print(f"{Fore.RED}üí• ERROR in {pipeline.upper()}")
        print(f"{Fore.RED}{'='*80}")
        print(f"{Fore.RED}‚ùå Error: {Style.BRIGHT}{error_message}")
        print(f"{Fore.RED}{'='*80}")
        
    def log_comparison_start(self, query: str):
        """Start comparison logging"""
        print(f"\n{Fore.MAGENTA}{'='*80}")
        print(f"{Fore.MAGENTA}üîÑ PIPELINE COMPARISON")
        print(f"{Fore.MAGENTA}{'='*80}")
        print(f"{Fore.YELLOW}üìù Query: '{query}'")
        
    def log_comparison_result(self, winner: str, p1_time: float, p2_time: float):
        """Display comparison results"""
        print(f"\n{Fore.GREEN}üèÜ Winner: {Style.BRIGHT}{winner}")
        print(f"{Fore.BLUE}‚ö° Pipeline 1: {p1_time:.2f}ms")
        print(f"{Fore.BLUE}‚ö° Pipeline 2: {p2_time:.2f}ms")
        print(f"{Fore.MAGENTA}{'='*80}")

class ExecutionMetrics:
    """Comprehensive metrics collection for pipeline evaluation"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.start_time = None
        self.end_time = None
        self.gpu_memory_start = 0
        self.gpu_memory_peak = 0
        self.cpu_percent_start = 0
        self.inference_times = []
        self.memory_snapshots = []
        
    def start_measurement(self):
        """Start measuring execution metrics"""
        self.reset()
        self.start_time = time.time()
        self.cpu_percent_start = psutil.cpu_percent(interval=None)
        
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            self.gpu_memory_start = torch.cuda.memory_allocated()
        
        # Start memory tracing
        tracemalloc.start()
        
    def record_inference_time(self, operation: str, duration: float):
        """Record individual inference operation time"""
        self.inference_times.append({
            'operation': operation,
            'duration_ms': duration * 1000,
            'timestamp': datetime.now().isoformat()
        })
        
    def end_measurement(self):
        """End measurement and calculate final metrics"""
        self.end_time = time.time()
        
        if torch.cuda.is_available():
            self.gpu_memory_peak = torch.cuda.max_memory_allocated()
        
        # Stop memory tracing
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        return self.get_metrics()
    
    def get_metrics(self):
        """Get comprehensive computation efficiency metrics"""
        total_time = (self.end_time - self.start_time) if self.end_time else 0
        
        return {
            'execution_time_ms': total_time * 1000,
            'gpu_memory_used_mb': (self.gpu_memory_peak - self.gpu_memory_start) / (1024**2) if torch.cuda.is_available() else 0,
            'gpu_memory_peak_mb': self.gpu_memory_peak / (1024**2) if torch.cuda.is_available() else 0,
            'cpu_usage_percent': psutil.cpu_percent(interval=None),
            'inference_breakdown': self.inference_times,
            'total_inference_operations': len(self.inference_times),
            'device': str(device),
            'timestamp': datetime.now().isoformat()
        }

class SQLEvaluator:
    """Evaluate SQL query accuracy and correctness"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or self.create_sample_db()
        
    def create_sample_db(self):
        """Create sample database for evaluation"""
        import sqlite3
        import tempfile
        
        db_path = '/tmp/sample_ecommerce.db'
        conn = sqlite3.connect(db_path)
        
        # Create sample tables matching Tiki dataset
        conn.execute('''
            CREATE TABLE IF NOT EXISTS products (
                id INTEGER PRIMARY KEY,
                name TEXT,
                brand TEXT,
                category TEXT,
                price REAL,
                rating REAL,
                review_count INTEGER
            )
        ''')
        
        # Insert sample data
        sample_data = [
            (1, 'Samsung Galaxy S21', 'Samsung', 'ƒëi·ªán tho·∫°i', 15000000, 4.5, 1200),
            (2, 'iPhone 13 Pro', 'Apple', 'ƒëi·ªán tho·∫°i', 25000000, 4.8, 2500),
            (3, 'T√∫i x√°ch Louis Vuitton', 'Louis Vuitton', 't√∫i x√°ch', 50000000, 4.9, 150),
            (4, 'Laptop Dell XPS 13', 'Dell', 'laptop', 30000000, 4.6, 800),
            (5, 'Tai nghe Sony WH-1000XM4', 'Sony', 'tai nghe', 8000000, 4.7, 950)
        ]
        
        conn.executemany('''
            INSERT OR REPLACE INTO products (id, name, brand, category, price, rating, review_count)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', sample_data)
        
        conn.commit()
        conn.close()
        return db_path
    
    def execute_sql(self, sql_query: str) -> Dict[str, Any]:
        """Execute SQL query and return results with metadata"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            start_time = time.time()
            cursor.execute(sql_query)
            results = cursor.fetchall()
            execution_time = time.time() - start_time
            
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            conn.close()
            
            return {
                'success': True,
                'results': results,
                'columns': columns,
                'row_count': len(results),
                'execution_time_ms': execution_time * 1000,
                'error': None
            }
            
        except Exception as e:
            return {
                'success': False,
                'results': [],
                'columns': [],
                'row_count': 0,
                'execution_time_ms': 0,
                'error': str(e)
            }
    
    def calculate_execution_accuracy(self, generated_sql: str, expected_results: List = None) -> Dict[str, Any]:
        """Calculate Execution Accuracy (EX) - whether SQL returns correct results"""
        execution_result = self.execute_sql(generated_sql)
        
        if not execution_result['success']:
            return {
                'ex_score': 0.0,
                'executable': False,
                'error': execution_result['error'],
                'result_count': 0
            }
        
        # If we have expected results, compare them
        if expected_results is not None:
            results_match = execution_result['results'] == expected_results
            ex_score = 1.0 if results_match else 0.0
        else:
            # If no expected results, just check if query executed successfully
            ex_score = 1.0 if execution_result['success'] else 0.0
        
        return {
            'ex_score': ex_score,
            'executable': execution_result['success'],
            'error': execution_result['error'],
            'result_count': execution_result['row_count'],
            'execution_time_ms': execution_result['execution_time_ms']
        }
    
    def calculate_exact_match(self, generated_sql: str, gold_sql: str) -> Dict[str, Any]:
        """Calculate Exact Match (EM) - syntactic equivalence with gold standard"""
        
        def normalize_sql(sql: str) -> str:
            """Normalize SQL for comparison"""
            # Convert to lowercase
            sql = sql.lower().strip()
            # Remove extra whitespace
            sql = re.sub(r'\s+', ' ', sql)
            # Remove semicolon at end
            sql = sql.rstrip(';')
            # Standardize quotes
            sql = sql.replace('"', "'")
            return sql
        
        normalized_generated = normalize_sql(generated_sql)
        normalized_gold = normalize_sql(gold_sql)
        
        exact_match = normalized_generated == normalized_gold
        
        # Calculate similarity score using character-level comparison
        if len(normalized_gold) == 0:
            similarity = 1.0 if len(normalized_generated) == 0 else 0.0
        else:
            # Simple character-based similarity
            common_chars = sum(1 for a, b in zip(normalized_generated, normalized_gold) if a == b)
            max_len = max(len(normalized_generated), len(normalized_gold))
            similarity = common_chars / max_len if max_len > 0 else 0.0
        
        return {
            'em_score': 1.0 if exact_match else 0.0,
            'exact_match': exact_match,
            'similarity_score': similarity,
            'normalized_generated': normalized_generated,
            'normalized_gold': normalized_gold,
            'length_difference': abs(len(normalized_generated) - len(normalized_gold))
        }

class PipelineLogger:
    """Comprehensive logging for pipeline evaluation"""
    
    def __init__(self):
        self.metrics = ExecutionMetrics()
        self.evaluator = SQLEvaluator()
        self.logs = []
        
    def start_pipeline_execution(self, pipeline_name: str, query: str):
        """Start logging pipeline execution"""
        self.current_log = {
            'pipeline': pipeline_name,
            'query': query,
            'query_hash': hashlib.md5(query.encode()).hexdigest()[:8],
            'start_time': datetime.now().isoformat(),
            'steps': [],
            'errors': []
        }
        self.metrics.start_measurement()
        
    def log_step(self, step_name: str, input_data: Any, output_data: Any, duration: float):
        """Log individual pipeline step"""
        step_log = {
            'step': step_name,
            'duration_ms': duration * 1000,
            'input_length': len(str(input_data)) if input_data else 0,
            'output_length': len(str(output_data)) if output_data else 0,
            'timestamp': datetime.now().isoformat()
        }
        
        self.current_log['steps'].append(step_log)
        self.metrics.record_inference_time(step_name, duration)
        
    def log_error(self, error_msg: str, step: str = None):
        """Log error during execution"""
        error_log = {
            'error': error_msg,
            'step': step,
            'timestamp': datetime.now().isoformat()
        }
        self.current_log['errors'].append(error_log)
        
    def end_pipeline_execution(self, final_sql: str, gold_sql: str = None) -> Dict[str, Any]:
        """End logging and calculate all metrics"""
        computation_metrics = self.metrics.end_measurement()
        
        # Calculate EX and EM scores
        ex_metrics = self.evaluator.calculate_execution_accuracy(final_sql)
        em_metrics = self.evaluator.calculate_exact_match(final_sql, gold_sql) if gold_sql else {
            'em_score': 0.0, 'exact_match': False, 'similarity_score': 0.0
        }
        
        # Complete the log
        self.current_log.update({
            'end_time': datetime.now().isoformat(),
            'final_sql': final_sql,
            'gold_sql': gold_sql,
            'computation_metrics': computation_metrics,
            'execution_accuracy': ex_metrics,
            'exact_match': em_metrics,
            'overall_success': ex_metrics['executable'] and len(self.current_log['errors']) == 0
        })
        
        self.logs.append(self.current_log.copy())
        
        return {
            'execution_log': self.current_log,
            'metrics_summary': {
                'execution_time_ms': computation_metrics['execution_time_ms'],
                'ex_score': ex_metrics['ex_score'],
                'em_score': em_metrics['em_score'],
                'executable': ex_metrics['executable'],
                'exact_match': em_metrics['exact_match'],
                'gpu_memory_mb': computation_metrics['gpu_memory_peak_mb'],
                'inference_steps': len(self.current_log['steps']),
                'error_count': len(self.current_log['errors'])
            }
        }
    
    def get_session_summary(self) -> Dict[str, Any]:
        """Get summary of all logged executions"""
        if not self.logs:
            return {'total_executions': 0}
            
        ex_scores = [log['execution_accuracy']['ex_score'] for log in self.logs]
        em_scores = [log['exact_match']['em_score'] for log in self.logs]
        execution_times = [log['computation_metrics']['execution_time_ms'] for log in self.logs]
        
        return {
            'total_executions': len(self.logs),
            'average_ex_score': sum(ex_scores) / len(ex_scores),
            'average_em_score': sum(em_scores) / len(em_scores),
            'average_execution_time_ms': sum(execution_times) / len(execution_times),
            'success_rate': sum(1 for log in self.logs if log['overall_success']) / len(self.logs),
            'total_errors': sum(len(log['errors']) for log in self.logs)
        }

# Cell 3 - Code: PhoBERT Training for Vietnamese NL2SQL
from typing import Dict, List, Any
import re
from enum import Enum
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from datasets import Dataset as HFDataset
import random

# Training Data Generation
class VietnameseNL2SQLDataGenerator:
    """Generate Vietnamese NL2SQL training pairs"""
    
    def __init__(self):
        self.vietnamese_templates = {
            # SIMPLE QUERIES - Basic keyword matching
            "product_search": [
                "T√¨m {product_type}",
                "Hi·ªÉn th·ªã {product_type}",
                "Cho t√¥i xem {product_type}",
                "T√¨m ki·∫øm {product_type}",
                "Li·ªát k√™ {product_type}",
                "T√¥i mu·ªën mua {product_type}",
                "C√≥ {product_type} n√†o kh√¥ng?",
                "Xem {product_type}",
                "T√¨m {product_type} cho t√¥i"
            ],
            
            "brand_filter": [
                "T√¨m {product_type} c·ªßa th∆∞∆°ng hi·ªáu {brand}",
                "Hi·ªÉn th·ªã {product_type} {brand}",
                "Cho t√¥i xem {product_type} h√£ng {brand}",
                "T√¨m {product_type} nh√£n hi·ªáu {brand}",
                "{product_type} c·ªßa {brand}",
                "{product_type} {brand}",
                "T√¥i mu·ªën {product_type} h√£ng {brand}",
                "C√≥ {product_type} {brand} kh√¥ng?",
                "Xem {product_type} th∆∞∆°ng hi·ªáu {brand}",
                "T√¨m {product_type} brand {brand}"
            ],
            "price_filter": [
                "{product_type} gi√° d∆∞·ªõi {price}k",
                "t√¨m {product_type} d∆∞·ªõi {price} tri·ªáu",
                "{product_type} gi√° r·∫ª d∆∞·ªõi {price} ngh√¨n",
                "s·∫£n ph·∫©m d∆∞·ªõi {price}k",
                "{product_type} kh√¥ng qu√° {price} tri·ªáu",
                "{product_type} gi√° d∆∞·ªõi {price} ngh√¨n",
                "{product_type} gi√° t·ª´ {price}k tr·ªü xu·ªëng",
                "T√¨m {product_type} c√≥ gi√° kh√¥ng v∆∞·ª£t qu√° {price} tri·ªáu",
                "{product_type} trong t·∫ßm gi√° {price}k",
                "Hi·ªÉn th·ªã {product_type} gi√° t·ªëi ƒëa {price} tri·ªáu"
            ],
            
            # MEDIUM QUERIES - Multiple conditions
            "multi_condition": [
                "{product_type} {gender} {color} c√≥ h∆°n {review_count} l∆∞·ª£t ƒë√°nh gi√°",
                "{product_type} {gender} {color} gi√° d∆∞·ªõi {price}k",
                "{product_type} {gender} {material} c√≥ ƒë√°nh gi√° tr√™n {rating} sao",
                "{product_type} {gender} {color} c√≥ h∆°n {sales_count} l∆∞·ª£t b√°n",
                "{product_type} {brand} {color} c√≥ ƒë√°nh gi√° cao",
                "{product_type} {gender} {material} gi√° h·ª£p l√Ω",
                "T√¨m {product_type} {brand} {color} size {size}",
                "{product_type} {gender} {material} ƒë√°nh gi√° t·ª´ {rating} sao tr·ªü l√™n",
                "Hi·ªÉn th·ªã {product_type} {brand} gi√° t·ª´ {min_price}k ƒë·∫øn {max_price}k",
                "{product_type} {color} {material} c√≥ nhi·ªÅu ng∆∞·ªùi mua",
                "{product_type} {brand} {gender} c√≥ nhi·ªÅu size"
            ],
            "logical_or": [
                "{product_type} {color} ho·∫∑c {color2}",
                "{product_type} h√£ng {brand} ho·∫∑c {brand2}",
                "{product_type} {gender} {color} ho·∫∑c {color2}",
                "T√¨m {product_type} {brand} ho·∫∑c {brand2}",
                "{product_type} m√†u {color} ho·∫∑c m√†u {color2}",
                "Hi·ªÉn th·ªã {product_type} {brand} ho·∫∑c th∆∞∆°ng hi·ªáu {brand2}",
                "{product_type} {material} ho·∫∑c {color}",
                "T√¨m {product_type} size {size} ho·∫∑c m√†u {color}",
                "{product_type} {gender} ho·∫∑c {color} {material}",
                "Cho t√¥i xem {product_type} {brand} ho·∫∑c gi√° d∆∞·ªõi {price}k"
            ],
            "price_range": [
                "{product_type} gi√° t·ª´ {min_price}k ƒë·∫øn {max_price}k",
                "T√¨m {product_type} trong kho·∫£ng {min_price} - {max_price} tri·ªáu",
                "{product_type} gi√° t·ª´ {min_price} ƒë·∫øn {max_price} ngh√¨n",
                "Hi·ªÉn th·ªã {product_type} gi√° {min_price}k - {max_price}k",
                "{product_type} trong t·∫ßm gi√° {min_price} - {max_price} tri·ªáu",
                "T√¨m ki·∫øm {product_type} c√≥ gi√° t·ª´ {min_price}k t·ªõi {max_price}k",
                "{product_type} {brand} gi√° kho·∫£ng {min_price} - {max_price} tri·ªáu",
                "Cho t√¥i xem {product_type} {gender} gi√° t·ª´ {min_price}k ƒë·∫øn {max_price}k",
                "Hi·ªÉn th·ªã {product_type} {color} trong kho·∫£ng gi√° {min_price} - {max_price} tri·ªáu",
                "{product_type} {material} c√≥ gi√° t·ª´ {min_price}k t·ªõi {max_price}k"
            ],
            "rating_filter": [
                "{product_type} c√≥ ƒë√°nh gi√° tr√™n {rating} sao",
                "t√¨m {product_type} rating tr√™n {rating} sao",
                "{product_type} c√≥ h∆°n {review_count} l∆∞·ª£t ƒë√°nh gi√°",
                "{product_type} c√≥ ƒë√°nh gi√° cao nh·∫•t",
                "{product_type} c√≥ rating cao nh·∫•t",
                "{product_type} ƒë∆∞·ª£c y√™u th√≠ch nhi·ªÅu nh·∫•t",
                "{product_type} c√≥ nhi·ªÅu ƒë√°nh gi√° t√≠ch c·ª±c",
                "{product_type} c√≥ s·ªë sao cao"
            ],
            
            # COMPLEX QUERIES - Aggregation and advanced operations
            "max_aggregation": [
                "s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c gi·∫£m gi√° nhi·ªÅu nh·∫•t",
                "{product_type} n√†o c√≥ gi√° cao nh·∫•t trong danh m·ª•c {gender}",
                "{product_type} n√†o c√≥ ƒë√°nh gi√° cao nh·∫•t",
                "th∆∞∆°ng hi·ªáu n√†o c√≥ {product_type} ƒë·∫Øt nh·∫•t",
                "{product_type} {brand} n√†o c√≥ gi√° cao nh·∫•t",
                "s·∫£n ph·∫©m n√†o c√≥ nhi·ªÅu l∆∞·ª£t xem nh·∫•t",
                "{product_type} n√†o ƒë∆∞·ª£c y√™u th√≠ch nh·∫•t",
                "t√¨m {product_type} c√≥ rating cao nh·∫•t",
                "{product_type} {gender} n√†o c√≥ gi√° tr·ªã cao nh·∫•t",
                "s·∫£n ph·∫©m n√†o c√≥ nhi·ªÅu ƒë√°nh gi√° nh·∫•t",
                "{product_type} n√†o c√≥ s·ªë l∆∞·ª£ng b√°n ch·∫°y nh·∫•t",
                "{product_type} n√†o c√≥ ƒë√°nh gi√° cao nh·∫•t",
                "{product_type} n√†o c√≥ nhi·ªÅu h√¨nh ·∫£nh nh·∫•t",
                "s·∫£n ph·∫©m n√†o c√≥ cashback cao nh·∫•t",
                "{product_type} n√†o c√≥ review nhi·ªÅu nh·∫•t"
            ],
            "top_products": [
                "t√¨m {limit} {product_type} {gender} c√≥ ƒë√°nh gi√° cao nh·∫•t trong danh m·ª•c {category}",
                "top {limit} {product_type} {gender} b√°n ch·∫°y nh·∫•t c√≥ gi√° d∆∞·ªõi {price} tri·ªáu",
                "{limit} {product_type} {gender} ƒë·∫Øt nh·∫•t c√≥ ƒë√°nh gi√° tr√™n {rating} sao",
                "hi·ªÉn th·ªã {limit} {product_type} t·ªët nh·∫•t",
                "cho t√¥i xem {limit} {product_type} {brand} ƒë∆∞·ª£c ƒë√°nh gi√° cao nh·∫•t",
                "t√¨m {limit} {product_type} {color} c√≥ nhi·ªÅu ng∆∞·ªùi mua nh·∫•t",
                "top {limit} {product_type} {material} gi√° r·∫ª nh·∫•t",
                "{limit} {product_type} {gender} c√≥ rating cao nh·∫•t",
                "li·ªát k√™ {limit} {product_type} b√°n ch·∫°y nh·∫•t th√°ng n√†y",
                "hi·ªÉn th·ªã {limit} {product_type} {brand} m·ªõi nh·∫•t",
                "{limit} {product_type} t·ªët nh·∫•t theo ƒë√°nh gi√° kh√°ch h√†ng",
                "top {limit} {product_type} {gender} {brand} c√≥ nhi·ªÅu size nh·∫•t",
                "{limit} {product_type} {gender} ƒë∆∞·ª£c y√™u th√≠ch nh·∫•t",
                "top {limit} s·∫£n ph·∫©m gi·∫£m gi√° s√¢u nh·∫•t h√¥m nay"
            ],
            "count_products": [
                "c√≥ bao nhi√™u s·∫£n ph·∫©m c·ªßa {gender} c√≥ gi√° t·ª´ {min_price}k ƒë·∫øn {max_price}k",
                "ƒë·∫øm s·ªë l∆∞·ª£ng {product_type} {gender} c√≥ ƒë√°nh gi√° tr√™n {rating} sao",
                "c√≥ bao nhi√™u {product_type} {material} gi√° d∆∞·ªõi {price} tri·ªáu",
                "t·ªïng s·ªë {product_type} {gender} c√≥ trong kho",
                "ƒë·∫øm s·ªë s·∫£n ph·∫©m {brand} c√≥ gi√° tr√™n {price}k",
                "c√≥ bao nhi√™u {product_type} {color}",
                "s·ªë l∆∞·ª£ng {product_type} c√≥ size {size}"
            ],
            "group_by_aggregation": [
                "hi·ªÉn th·ªã c√°c lo·∫°i {product_type} c√≥ gi√° trung b√¨nh tr√™n {price}k",
                "th·ªëng k√™ s·ªë l∆∞·ª£ng s·∫£n ph·∫©m theo t·ª´ng th∆∞∆°ng hi·ªáu",
                "c√°c danh m·ª•c n√†o c√≥ gi√° trung b√¨nh cao nh·∫•t",
                "th∆∞∆°ng hi·ªáu n√†o c√≥ nhi·ªÅu s·∫£n ph·∫©m nh·∫•t",
                "lo·∫°i {product_type} n√†o c√≥ ƒë√°nh gi√° trung b√¨nh t·ªët nh·∫•t",
                "danh m·ª•c n√†o c√≥ s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t",
                "th·ªëng k√™ gi√° trung b√¨nh theo t·ª´ng lo·∫°i {product_type}",
                "c√°c th∆∞∆°ng hi·ªáu c√≥ rating trung b√¨nh tr√™n {rating}"
            ],
            "complex_filtering": [
                "s·∫£n ph·∫©m n√†o c√≥ t·ª∑ l·ªá gi·∫£m gi√° cao nh·∫•t so v·ªõi gi√° g·ªëc",
                "th∆∞∆°ng hi·ªáu n√†o c√≥ s·∫£n ph·∫©m v·ªõi cashback trung b√¨nh cao nh·∫•t",
                "danh m·ª•c n√†o c√≥ s·∫£n ph·∫©m ƒë∆∞·ª£c review nhi·ªÅu nh·∫•t",
                "lo·∫°i s·∫£n ph·∫©m n√†o c√≥ s·ªë l∆∞·ª£ng h√¨nh ·∫£nh trung b√¨nh nhi·ªÅu nh·∫•t",
                "c√°c s·∫£n ph·∫©m c√≥ t·ª∑ l·ªá y√™u th√≠ch cao nh·∫•t",
                "th∆∞∆°ng hi·ªáu n√†o c√≥ s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t th√°ng n√†y",
                "ph√¢n t√≠ch xu h∆∞·ªõng gi√° theo t·ª´ng lo·∫°i s·∫£n ph·∫©m"
            ]
        }
        
        self.sql_templates = {
            # SIMPLE QUERIES
            "product_search": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%'",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%'",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{color}%'",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{material}%'"
            ],
            "brand_filter": [
                "SELECT * FROM products WHERE brand LIKE '%{brand}%' AND name LIKE '%{product_type}%'",
                "SELECT * FROM products WHERE brand LIKE '%{brand}%' AND name LIKE '%{product_type}%' AND name LIKE '%{gender}%'",
                "SELECT * FROM products WHERE brand LIKE '%{brand}%' AND name LIKE '%{product_type}%' AND name LIKE '%{color}%'"
            ],
            "price_filter": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND price <= {price}",
                "SELECT * FROM products WHERE price <= {price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND price <= {price} ORDER BY price ASC"
            ],
            
            # MEDIUM QUERIES
            "multi_condition": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND name LIKE '%{color}%' AND review_count > {review_count}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND name LIKE '%{color}%' AND price <= {price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND name LIKE '%{material}%' AND rating >= {rating}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' AND name LIKE '%{color}%' AND rating >= 4.0",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND name LIKE '%{material}%' AND price <= 1000000",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' AND name LIKE '%{color}%' AND name LIKE '%{size}%'",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND name LIKE '%{material}%' AND rating >= {rating}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{color}%' AND name LIKE '%{material}%' AND review_count > 100"
            ],
            "logical_or": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (name LIKE '%{color}%' OR name LIKE '%{color2}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (brand LIKE '%{brand}%' OR brand LIKE '%{brand2}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND (name LIKE '%{color}%' OR name LIKE '%{color2}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (brand LIKE '%{brand}%' OR brand LIKE '%{brand2}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (name LIKE '%{material}%' OR name LIKE '%{color}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (name LIKE '%{size}%' OR name LIKE '%{color}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND (name LIKE '%{color}%' OR name LIKE '%{material}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND (brand LIKE '%{brand}%' OR price <= {price})"
            ],
            "price_range": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND price BETWEEN {min_price} AND {max_price} ORDER BY price ASC",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' AND price BETWEEN {min_price} AND {max_price} ORDER BY rating DESC",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{color}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{material}%' AND price BETWEEN {min_price} AND {max_price}"
            ],
            "rating_filter": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND rating >= {rating}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND review_count > {review_count}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' ORDER BY rating DESC LIMIT 10",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND rating >= 4.5 ORDER BY rating DESC",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND review_count > 100 ORDER BY review_count DESC"
            ],
            
            # COMPLEX QUERIES
            "max_aggregation": [
                "SELECT * FROM products WHERE discount_rate = (SELECT MAX(discount_rate) FROM products)",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND price = (SELECT MAX(price) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND review_count = (SELECT MAX(review_count) FROM products WHERE name LIKE '%{product_type}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND rating = (SELECT MAX(rating) FROM products WHERE name LIKE '%{product_type}%')",
                "SELECT * FROM products WHERE image_count = (SELECT MAX(image_count) FROM products)",
                "SELECT * FROM products WHERE cashback_rate = (SELECT MAX(cashback_rate) FROM products)",
                "SELECT brand, MAX(price) FROM products WHERE name LIKE '%{product_type}%' GROUP BY brand ORDER BY MAX(price) DESC LIMIT 1",
                "SELECT * FROM products WHERE view_count = (SELECT MAX(view_count) FROM products)",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND favorite_count = (SELECT MAX(favorite_count) FROM products WHERE name LIKE '%{product_type}%')",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND price = (SELECT MAX(price) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%')"
            ],
            "top_products": [
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND category LIKE '%{category}%' ORDER BY rating DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND price <= {price} ORDER BY review_count DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND rating >= {rating} ORDER BY price DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' ORDER BY rating DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' ORDER BY rating DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{color}%' ORDER BY review_count DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{material}%' ORDER BY price ASC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' ORDER BY rating DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND MONTH(created_date) = MONTH(CURRENT_DATE) ORDER BY review_count DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND brand LIKE '%{brand}%' ORDER BY created_date DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' ORDER BY (favorite_count / review_count) DESC LIMIT {limit}",
                "SELECT * FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND brand LIKE '%{brand}%' ORDER BY size_count DESC LIMIT {limit}",
                "SELECT * FROM products WHERE discount_rate > 0 ORDER BY discount_rate DESC LIMIT {limit}"
            ],
            "count_products": [
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{gender}%' AND price BETWEEN {min_price} AND {max_price}",
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%' AND rating >= {rating}",
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{material}%' AND price <= {price}",
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{gender}%'",
                "SELECT COUNT(*) FROM products WHERE brand LIKE '%{brand}%' AND price >= {price}",
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{product_type}%' AND name LIKE '%{color}%'",
                "SELECT COUNT(*) FROM products WHERE name LIKE '%{product_type}%' AND size LIKE '%{size}%'"
            ],
            "group_by_aggregation": [
                "SELECT category, AVG(price) FROM products WHERE name LIKE '%{product_type}%' GROUP BY category HAVING AVG(price) > {price}",
                "SELECT brand, COUNT(*) FROM products GROUP BY brand ORDER BY COUNT(*) DESC",
                "SELECT category, AVG(price) FROM products GROUP BY category ORDER BY AVG(price) DESC",
                "SELECT brand, COUNT(*) FROM products GROUP BY brand ORDER BY COUNT(*) DESC LIMIT 1",
                "SELECT category, AVG(rating) FROM products WHERE name LIKE '%{product_type}%' GROUP BY category ORDER BY AVG(rating) DESC",
                "SELECT category, COUNT(*) FROM products GROUP BY category ORDER BY COUNT(*) DESC LIMIT 1",
                "SELECT category, AVG(price) FROM products WHERE name LIKE '%{product_type}%' GROUP BY category",
                "SELECT brand, AVG(rating) FROM products GROUP BY brand HAVING AVG(rating) >= {rating}"
            ],
            "complex_filtering": [
                "SELECT *, (original_price - price) / original_price * 100 as discount_percent FROM products ORDER BY discount_percent DESC LIMIT 1",
                "SELECT brand, AVG(cashback_rate) FROM products GROUP BY brand ORDER BY AVG(cashback_rate) DESC LIMIT 1",
                "SELECT category, SUM(review_count) FROM products GROUP BY category ORDER BY SUM(review_count) DESC LIMIT 1",
                "SELECT category, AVG(image_count) FROM products GROUP BY category ORDER BY AVG(image_count) DESC LIMIT 1",
                "SELECT *, favorite_count / review_count * 100 as favorite_rate FROM products ORDER BY favorite_rate DESC LIMIT 10",
                "SELECT brand, SUM(review_count) FROM products WHERE MONTH(created_date) = MONTH(CURRENT_DATE) GROUP BY brand ORDER BY SUM(review_count) DESC LIMIT 1",
                "SELECT category, AVG(price) FROM products GROUP BY category ORDER BY AVG(price)"
            ]
        }
        
        self.entities = {
            # Expanded product types to match test queries
            "product_types": [
                "√°o thun", "gi√†y", "balo", "t√∫i x√°ch", "v√≠", "d√©p", "sandal", "sneaker", "boot", 
                "ph·ª• ki·ªán", "√°o kho√°c", "qu·∫ßn jean", "v√°y", "√°o s∆° mi", "hoodie", "polo",
                "gi√†y th·ªÉ thao", "gi√†y cao g√≥t", "gi√†y b√∫p b√™", "t√∫i ƒëeo ch√©o", "t√∫i ƒëeo vai",
                "balo laptop", "balo du l·ªãch", "ƒëi·ªán tho·∫°i", "laptop", "tai nghe", "ƒë·ªìng h·ªì"
            ],
            
            # Expanded brands to match test queries
            "brands": [
                "Nike", "Adidas", "Samsung", "Apple", "Sony", "Gucci", "Louis Vuitton", 
                "Converse", "Vans", "Jansport", "Herschel", "Uniqlo", "Coach", "Birkenstock",
                "Havaianas", "Dell", "HP", "Xiaomi"
            ],
            
            # Gender specifications
            "genders": ["nam", "n·ªØ"],
            
            # Colors from test queries
            "colors": ["ƒëen", "tr·∫Øng", "n√¢u", "xanh", "ƒë·ªè", "x√°m"],
            
            # Materials from test queries
            "materials": ["da", "cotton", "v·∫£i", "nh·ª±a", "da th·∫≠t", "da b√≤", "denim"],
            
            # Price ranges (in VND)
            "prices_k": [50, 100, 150, 200, 300, 500],  # thousands
            "prices_tri·ªáu": [1, 1.5, 2, 3, 5, 10, 15, 20, 25, 30],  # millions
            
            # Ratings
            "ratings": [3.5, 4.0, 4.5, 5.0],
            
            # Limits for TOP queries
            "limits": [3, 5, 7, 8, 10, 15, 20],
            
            # Review counts
            "review_counts": [50, 100, 200, 500],
            
            # Sales counts
            "sales_counts": [50, 100, 200],
            
            # Sizes
            "sizes": ["37", "38", "39", "40", "41", "42", "S", "M", "L", "XL"],
            
            # Categories
            "categories": [
                "Th·ªùi Trang Nam", "Th·ªùi Trang N·ªØ", "Gi√†y D√©p Nam", "Gi√†y D√©p N·ªØ",
                "T√∫i V√≠ Nam", "T√∫i V√≠ N·ªØ", "Ph·ª• Ki·ªán Th·ªùi Trang"
            ]
        }
    
    def generate_training_pairs(self, num_samples: int = 5000) -> List[Dict[str, str]]:
        """Generate Vietnamese NL2SQL training pairs with improved coverage"""
        training_data = []
        
        # Ensure balanced distribution across complexity levels
        simple_types = ["product_search", "brand_filter", "price_filter"]
        medium_types = ["multi_condition", "logical_or", "price_range", "rating_filter"]
        complex_types = ["max_aggregation", "top_products", "count_products", "group_by_aggregation", "complex_filtering"]
        
        # Allocate samples: 40% simple, 35% medium, 25% complex
        simple_count = int(num_samples * 0.4)
        medium_count = int(num_samples * 0.35)
        complex_count = num_samples - simple_count - medium_count
        
        # Generate simple queries
        for _ in range(simple_count):
            template_type = random.choice(simple_types)
            self._generate_single_pair(template_type, training_data)
        
        # Generate medium queries
        for _ in range(medium_count):
            template_type = random.choice(medium_types)
            self._generate_single_pair(template_type, training_data)
        
        # Generate complex queries
        for _ in range(complex_count):
            template_type = random.choice(complex_types)
            self._generate_single_pair(template_type, training_data)
        
        print(f"Generated {len(training_data)} training pairs:")
        print(f"- Simple queries: {simple_count}")
        print(f"- Medium queries: {medium_count}")
        print(f"- Complex queries: {complex_count}")
        
        return training_data
    
    def _generate_single_pair(self, template_type: str, training_data: List[Dict]):
        """Generate a single Vietnamese-SQL pair"""
        try:
            vn_template = random.choice(self.vietnamese_templates[template_type])
            sql_template = random.choice(self.sql_templates[template_type])
            
            entities = self._generate_random_entities(template_type)
            
            vietnamese_query = vn_template.format(**entities)
            sql_query = sql_template.format(**entities)
            
            training_data.append({
                "vietnamese_query": vietnamese_query,
                "sql_query": sql_query,
                "template_type": template_type,
                "complexity": self._get_complexity_level(template_type)
            })
        except (KeyError, IndexError) as e:
            # Skip malformed templates
            pass
    
    def _get_complexity_level(self, template_type: str) -> str:
        """Get complexity level for a template type"""
        simple_types = ["product_search", "brand_filter", "price_filter"]
        medium_types = ["multi_condition", "logical_or", "price_range", "rating_filter"]
        complex_types = ["max_aggregation", "top_products", "count_products", "group_by_aggregation", "complex_filtering"]
        
        if template_type in simple_types:
            return "simple"
        elif template_type in medium_types:
            return "medium"
        else:
            return "complex"
    
    def _generate_random_entities(self, template_type: str) -> Dict[str, Any]:
        """Generate random entities for template filling based on test query patterns"""
        entities = {}
        
        # Base product information
        entities["product_type"] = random.choice(self.entities["product_types"])
        entities["category"] = entities["product_type"]
        
        # Brand information
        if template_type in ["brand_filter", "multi_condition", "logical_or", "complex_filtering"]:
            entities["brand"] = random.choice(self.entities["brands"])
            entities["brand2"] = random.choice([b for b in self.entities["brands"] if b != entities["brand"]])
        
        # Price information (handle both thousands and millions)
        if template_type in ["price_filter", "price_range", "multi_condition", "complex_filtering"]:
            # Randomly choose between thousand and million price ranges
            if random.choice([True, False]):
                entities["price"] = random.choice(self.entities["prices_k"])
                entities["price_unit"] = "k"
                entities["min_price"] = random.choice(self.entities["prices_k"])
                entities["max_price"] = entities["min_price"] + random.choice([50, 100, 200])
            else:
                entities["price"] = random.choice(self.entities["prices_tri·ªáu"])
                entities["price_unit"] = "tri·ªáu"
                entities["min_price"] = random.choice(self.entities["prices_tri·ªáu"])
                entities["max_price"] = entities["min_price"] + random.choice([1, 2, 5])
        
        # Rating information
        if template_type in ["rating_filter", "multi_condition", "complex_filtering"]:
            entities["rating"] = random.choice(self.entities["ratings"])
        
        # Gender information
        if template_type in ["multi_condition", "logical_or", "complex_filtering"]:
            entities["gender"] = random.choice(self.entities["genders"])
        
        # Color information
        if template_type in ["multi_condition", "logical_or", "complex_filtering"]:
            entities["color"] = random.choice(self.entities["colors"])
            entities["color2"] = random.choice([c for c in self.entities["colors"] if c != entities["color"]])
        
        # Material information
        if template_type in ["multi_condition", "complex_filtering"]:
            entities["material"] = random.choice(self.entities["materials"])
        
        # Size information
        if template_type in ["multi_condition", "complex_filtering"]:
            entities["size"] = random.choice(self.entities["sizes"])
        
        # Limit for TOP queries
        if template_type in ["top_products", "count_products", "max_aggregation", "group_by_aggregation"]:
            entities["limit"] = random.choice(self.entities["limits"])
        
        # Review and sales counts
        if template_type in ["complex_filtering"]:
            entities["review_count"] = random.choice(self.entities["review_counts"])
            entities["sales_count"] = random.choice(self.entities["sales_counts"])
        
        # Category information for aggregation queries
        if template_type in ["group_by_aggregation"]:
            entities["category"] = random.choice(self.entities["categories"])
        
        return entities

# Custom PhoBERT Model for SQL Generation
class PhoBERTForSQL(nn.Module):
    """PhoBERT with SQL generation head"""
    
    def __init__(self, model_name="vinai/phobert-base", max_sql_length=128):
        super().__init__()
        from transformers import AutoModel, AutoTokenizer
        
        self.phobert = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # SQL generation head
        self.sql_head = nn.Sequential(
            nn.Linear(self.phobert.config.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, self.tokenizer.vocab_size)
        )
        
        self.max_sql_length = max_sql_length
        
    def forward(self, input_ids, attention_mask=None, sql_labels=None):
        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        
        # Use CLS token for SQL generation
        cls_output = sequence_output[:, 0, :]
        sql_logits = self.sql_head(cls_output)
        
        loss = None
        if sql_labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            # Simple approach: predict first token of SQL
            loss = loss_fct(sql_logits, sql_labels[:, 0])
        
        return {
            "loss": loss,
            "logits": sql_logits,
            "hidden_states": sequence_output
        }
    
    def generate_sql(self, vietnamese_query: str, max_length: int = 128):
        """Generate SQL from Vietnamese query"""
        self.eval()
        
        inputs = self.tokenizer(
            vietnamese_query,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.forward(**inputs)
            logits = outputs["logits"]
            
            # Simple template-based generation using learned features
            predicted_class = torch.argmax(logits, dim=-1)
            
            # Map to SQL templates (hybrid approach)
            sql_query = self._map_to_sql_template(vietnamese_query, predicted_class.item())
            
        return sql_query
    
    def _map_to_sql_template(self, query: str, predicted_class: int):
        """Map PhoBERT prediction to SQL template"""
        query_lower = query.lower()
        
        # Brand detection
        brands = ["samsung", "apple", "sony", "dell", "hp", "xiaomi"]
        detected_brand = None
        for brand in brands:
            if brand in query_lower:
                detected_brand = brand.title()
                break
        
        # Category detection
        if any(word in query_lower for word in ["ƒëi·ªán tho·∫°i", "phone"]):
            category = "ƒëi·ªán tho·∫°i"
        elif any(word in query_lower for word in ["laptop", "m√°y t√≠nh"]):
            category = "laptop"
        elif any(word in query_lower for word in ["t√∫i", "balo"]):
            category = "t√∫i x√°ch"
        else:
            category = "ƒëi·ªán tho·∫°i"  # default
        
        # Price detection
        price_match = re.search(r'(\d+)\s*tri·ªáu', query_lower)
        price = int(price_match.group(1)) * 1000000 if price_match else None
        
        # Generate SQL based on detected entities
        if detected_brand and price:
            return f"SELECT * FROM products WHERE brand = '{detected_brand}' AND category = '{category}' AND price <= {price}"
        elif detected_brand:
            return f"SELECT * FROM products WHERE brand = '{detected_brand}' AND category = '{category}'"
        elif price:
            return f"SELECT * FROM products WHERE category = '{category}' AND price <= {price}"
        elif "top" in query_lower or "t·ªët nh·∫•t" in query_lower:
            return f"SELECT * FROM products WHERE category = '{category}' ORDER BY rating DESC LIMIT 10"
        elif "bao nhi√™u" in query_lower:
            return f"SELECT COUNT(*) FROM products WHERE category = '{category}'"
        else:
            return f"SELECT * FROM products WHERE category = '{category}'"

# Dataset Class
class VietnameseNL2SQLDataset(Dataset):
    """Dataset for Vietnamese NL2SQL training"""
    
    def __init__(self, data_pairs: List[Dict], tokenizer, max_length: int = 256):
        self.data_pairs = data_pairs
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        pair = self.data_pairs[idx]
        
        vietnamese_encoding = self.tokenizer(
            pair["vietnamese_query"],
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        sql_encoding = self.tokenizer(
            pair["sql_query"],
            truncation=True,
            padding="max_length", 
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        return {
            "input_ids": vietnamese_encoding["input_ids"].squeeze(),
            "attention_mask": vietnamese_encoding["attention_mask"].squeeze(),
            "sql_labels": sql_encoding["input_ids"].squeeze()
        }

# Training Function
def train_phobert_sql_model():
    """Train PhoBERT for Vietnamese NL2SQL"""
    print("üöÄ Starting PhoBERT training for Vietnamese NL2SQL...")
    
    # Generate training data
    data_generator = VietnameseNL2SQLDataGenerator()
    training_pairs = data_generator.generate_training_pairs(2000)
    print(f"‚úÖ Generated {len(training_pairs)} training pairs")
    
    # Initialize model
    model = PhoBERTForSQL()
    model.to(device)
    
    # Create datasets
    train_size = int(0.8 * len(training_pairs))
    train_data = training_pairs[:train_size]
    val_data = training_pairs[train_size:]
    
    train_dataset = VietnameseNL2SQLDataset(train_data, model.tokenizer)
    val_dataset = VietnameseNL2SQLDataset(val_data, model.tokenizer)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./phobert-sql-results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=100,
        save_steps=200,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        dataloader_pin_memory=False,
        report_to=[]  # Explicitly disable all reporting
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )
    
    # Train
    try:
        trainer.train()
        print("‚úÖ PhoBERT training completed successfully!")
        
        # Save model
        model.phobert.save_pretrained("./phobert-sql-final")
        model.tokenizer.save_pretrained("./phobert-sql-final")
        
        return model
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        return None

# Initialize training (set to False to skip training)
TRAIN_PHOBERT = True

if TRAIN_PHOBERT:
    print("üî• Training PhoBERT for Vietnamese NL2SQL...")
    trained_phobert_model = train_phobert_sql_model()
    if trained_phobert_model:
        print("üíæ PhoBERT model trained and ready!")
    else:
        print("‚ö†Ô∏è  Training failed, will use base PhoBERT")
        trained_phobert_model = PhoBERTForSQL()
        trained_phobert_model.to(device)
else:
    print("‚è≠Ô∏è  Skipping training, using base PhoBERT")
    trained_phobert_model = PhoBERTForSQL()
    trained_phobert_model.to(device)

print("‚úÖ PhoBERT-SQL model ready for Pipeline 1")

# Cell 4 - Code: SQL Template System (Backup)

class QueryIntent(Enum):
    """Query intent categories for Vietnamese e-commerce queries"""
    PRODUCT_SEARCH = "product_search"
    PRICE_FILTER = "price_filter"
    BRAND_FILTER = "brand_filter"
    CATEGORY_FILTER = "category_filter"
    RATING_FILTER = "rating_filter"
    SORT_PRODUCTS = "sort_products"
    COUNT_PRODUCTS = "count_products"
    COMPARISON = "comparison"
    TOP_PRODUCTS = "top_products"

class SQLTemplateBuilder:
    """Builds SQL queries from Vietnamese intents and entities"""
    
    def __init__(self):
        self.templates = self._initialize_templates()
        self.vietnamese_keywords = self._initialize_keywords()
        
    def _initialize_keywords(self) -> Dict[str, List[str]]:
        """Vietnamese keywords for entity extraction"""
        return {
            # Price keywords
            "price_low": ["r·∫ª", "gi√° r·∫ª", "gi√° th·∫•p", "d∆∞·ªõi", "nh·ªè h∆°n", "√≠t h∆°n", "kh√¥ng qu√°"],
            "price_high": ["ƒë·∫Øt", "gi√° cao", "tr√™n", "l·ªõn h∆°n", "nhi·ªÅu h∆°n", "t·ª´"],
            "price_range": ["t·ª´", "ƒë·∫øn", "trong kho·∫£ng", "gi·ªØa"],
            
            # Sort keywords
            "sort_price_asc": ["gi√° tƒÉng d·∫ßn", "gi√° t·ª´ th·∫•p ƒë·∫øn cao", "r·∫ª nh·∫•t"],
            "sort_price_desc": ["gi√° gi·∫£m d·∫ßn", "gi√° t·ª´ cao ƒë·∫øn th·∫•p", "ƒë·∫Øt nh·∫•t"],
            "sort_rating": ["ƒë√°nh gi√° cao", "rating cao", "t·ªët nh·∫•t"],
            "sort_popular": ["b√°n ch·∫°y", "ph·ªï bi·∫øn", "nhi·ªÅu ng∆∞·ªùi mua"],
            
            # Category keywords
            "categories": {
                "ƒëi·ªán tho·∫°i": ["ƒëi·ªán tho·∫°i", "smartphone", "phone", "di ƒë·ªông"],
                "laptop": ["laptop", "m√°y t√≠nh x√°ch tay", "notebook"],
                "t√∫i x√°ch": ["t√∫i x√°ch", "t√∫i", "balo", "c·∫∑p"],
                "th·ªùi trang": ["qu·∫ßn √°o", "th·ªùi trang", "√°o", "qu·∫ßn"],
                "ph·ª• ki·ªán": ["ph·ª• ki·ªán", "c√†i √°o", "trang s·ª©c"]
            },
            
            # Brand keywords
            "brands": ["Samsung", "Apple", "Xiaomi", "Oppo", "Vivo", "Sony", "LG", "Huawei"],
            
            # Count/Top keywords
            "count": ["c√≥ bao nhi√™u", "s·ªë l∆∞·ª£ng", "t·ªïng c·ªông"],
            "top": ["top", "h√†ng ƒë·∫ßu", "t·ªët nh·∫•t", "cao nh·∫•t", "nhi·ªÅu nh·∫•t"]
        }
    
    def _initialize_templates(self) -> Dict[QueryIntent, Dict[str, str]]:
        """SQL templates for each query intent"""
        return {
            QueryIntent.PRODUCT_SEARCH: {
                "basic": "SELECT * FROM products WHERE name LIKE '%{product_name}%'",
                "with_category": "SELECT * FROM products WHERE category = '{category}'",
                "with_brand": "SELECT * FROM products WHERE brand = '{brand}'"
            },
            
            QueryIntent.PRICE_FILTER: {
                "under": "SELECT * FROM products WHERE price <= {max_price}",
                "over": "SELECT * FROM products WHERE price >= {min_price}",
                "range": "SELECT * FROM products WHERE price BETWEEN {min_price} AND {max_price}",
                "with_category": "SELECT * FROM products WHERE price <= {max_price} AND category = '{category}'"
            },
            
            QueryIntent.BRAND_FILTER: {
                "basic": "SELECT * FROM products WHERE brand = '{brand}'",
                "with_category": "SELECT * FROM products WHERE brand = '{brand}' AND category = '{category}'",
                "with_price": "SELECT * FROM products WHERE brand = '{brand}' AND price <= {max_price}"
            },
            
            QueryIntent.CATEGORY_FILTER: {
                "basic": "SELECT * FROM products WHERE category = '{category}'",
                "with_price": "SELECT * FROM products WHERE category = '{category}' AND price <= {max_price}",
                "with_rating": "SELECT * FROM products WHERE category = '{category}' AND rating >= {min_rating}"
            },
            
            QueryIntent.TOP_PRODUCTS: {
                "by_rating": "SELECT * FROM products ORDER BY rating DESC LIMIT {limit}",
                "by_sales": "SELECT * FROM products ORDER BY review_count DESC LIMIT {limit}",
                "by_price_low": "SELECT * FROM products ORDER BY price ASC LIMIT {limit}",
                "by_category": "SELECT * FROM products WHERE category = '{category}' ORDER BY rating DESC LIMIT {limit}"
            },
            
            QueryIntent.COUNT_PRODUCTS: {
                "basic": "SELECT COUNT(*) as total_products FROM products",
                "by_category": "SELECT COUNT(*) as total_products FROM products WHERE category = '{category}'",
                "by_brand": "SELECT COUNT(*) as total_products FROM products WHERE brand = '{brand}'"
            }
        }
    
    def extract_entities(self, vietnamese_query: str) -> Dict[str, Any]:
        """Extract entities from Vietnamese query using keyword matching"""
        entities = {}
        query_lower = vietnamese_query.lower()
        
        # Extract price values
        price_pattern = r'(\d+(?:\.\d+)?)\s*(?:k|ngh√¨n|tri·ªáu|tr|ƒë·ªìng|vnd)?'
        prices = re.findall(price_pattern, query_lower)
        if prices:
            price_values = []
            for price in prices:
                value = float(price)
                if 'k' in query_lower or 'ngh√¨n' in query_lower:
                    value *= 1000
                elif 'tri·ªáu' in query_lower or 'tr' in query_lower:
                    value *= 1000000
                price_values.append(value)
            entities['prices'] = price_values
        
        # Extract categories
        for category, keywords in self.vietnamese_keywords['categories'].items():
            for keyword in keywords:
                if keyword in query_lower:
                    entities['category'] = category
                    break
        
        # Extract brands
        for brand in self.vietnamese_keywords['brands']:
            if brand.lower() in query_lower:
                entities['brand'] = brand
                break
        
        # Extract rating
        rating_pattern = r'(\d+(?:\.\d+)?)\s*(?:sao|star|rating)'
        ratings = re.findall(rating_pattern, query_lower)
        if ratings:
            entities['rating'] = float(ratings[0])
        
        # Extract limit for top queries
        limit_pattern = r'(\d+)\s*(?:s·∫£n ph·∫©m|sp|item)'
        limits = re.findall(limit_pattern, query_lower)
        if limits:
            entities['limit'] = int(limits[0])
        else:
            entities['limit'] = 10  # default
        
        return entities
    
    def classify_intent(self, vietnamese_query: str) -> QueryIntent:
        """Classify Vietnamese query intent"""
        query_lower = vietnamese_query.lower()
        
        # Check for count queries
        if any(keyword in query_lower for keyword in self.vietnamese_keywords['count']):
            return QueryIntent.COUNT_PRODUCTS
        
        # Check for top/best queries
        if any(keyword in query_lower for keyword in self.vietnamese_keywords['top']):
            return QueryIntent.TOP_PRODUCTS
        
        # Check for price filters
        if any(keyword in query_lower for keyword in self.vietnamese_keywords['price_low'] + self.vietnamese_keywords['price_high']):
            return QueryIntent.PRICE_FILTER
        
        # Check for brand filters
        if any(brand.lower() in query_lower for brand in self.vietnamese_keywords['brands']):
            return QueryIntent.BRAND_FILTER
        
        # Check for category filters
        if any(any(keyword in query_lower for keyword in keywords) 
               for keywords in self.vietnamese_keywords['categories'].values()):
            return QueryIntent.CATEGORY_FILTER
        
        # Default to product search
        return QueryIntent.PRODUCT_SEARCH
    
    def build_sql(self, vietnamese_query: str) -> Dict[str, Any]:
        """Build SQL query from Vietnamese natural language"""
        
        # Extract entities and classify intent
        entities = self.extract_entities(vietnamese_query)
        intent = self.classify_intent(vietnamese_query)
        
        # Get appropriate template
        template_group = self.templates[intent]
        
        # Select specific template based on available entities
        template_key = self._select_template(intent, entities)
        sql_template = template_group[template_key]
        
        # Fill template with entities
        try:
            sql_query = self._fill_template(sql_template, entities)
            
            return {
                "sql_query": sql_query,
                "intent": intent.value,
                "entities": entities,
                "template_used": template_key,
                "success": True
            }
        except Exception as e:
            return {
                "sql_query": None,
                "intent": intent.value,
                "entities": entities,
                "error": str(e),
                "success": False
            }
    
    def _select_template(self, intent: QueryIntent, entities: Dict[str, Any]) -> str:
        """Select most appropriate template based on available entities"""
        
        if intent == QueryIntent.PRODUCT_SEARCH:
            if 'category' in entities:
                return "with_category"
            elif 'brand' in entities:
                return "with_brand"
            return "basic"
        
        elif intent == QueryIntent.PRICE_FILTER:
            if 'category' in entities:
                return "with_category"
            elif len(entities.get('prices', [])) >= 2:
                return "range"
            return "under"
        
        elif intent == QueryIntent.BRAND_FILTER:
            if 'category' in entities:
                return "with_category"
            elif 'prices' in entities:
                return "with_price"
            return "basic"
        
        elif intent == QueryIntent.CATEGORY_FILTER:
            if 'prices' in entities:
                return "with_price"
            elif 'rating' in entities:
                return "with_rating"
            return "basic"
        
        elif intent == QueryIntent.TOP_PRODUCTS:
            if 'category' in entities:
                return "by_category"
            return "by_rating"
        
        elif intent == QueryIntent.COUNT_PRODUCTS:
            if 'category' in entities:
                return "by_category"
            elif 'brand' in entities:
                return "by_brand"
            return "basic"
        
        return "basic"
    
    def _fill_template(self, template: str, entities: Dict[str, Any]) -> str:
        """Fill SQL template with extracted entities"""
        
        # Prepare parameters for template filling
        params = {}
        
        if 'prices' in entities:
            prices = entities['prices']
            if len(prices) >= 2:
                params['min_price'] = min(prices)
                params['max_price'] = max(prices)
            elif len(prices) == 1:
                params['max_price'] = prices[0]
                params['min_price'] = prices[0]
        
        if 'category' in entities:
            params['category'] = entities['category']
        
        if 'brand' in entities:
            params['brand'] = entities['brand']
        
        if 'rating' in entities:
            params['min_rating'] = entities['rating']
        
        if 'limit' in entities:
            params['limit'] = entities['limit']
        
        # Handle product name extraction (placeholder)
        params['product_name'] = ""
        
        # Fill template
        return template.format(**params)

# Initialize SQL Template Builder
sql_template_builder = SQLTemplateBuilder()
print("‚úÖ SQL Template Builder initialized for Pipeline 1")

# Cell 4 - Code: Model Loading for Both Pipelines
from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, AutoModelForCausalLM
import os

class GoogleDriveManager:
    """Google Drive integration for model persistence"""
    
    def __init__(self):
        self.drive_root = '/content/drive/MyDrive/MSE_Thesis_Models'
        self.mounted = False
        self._mount_drive()
    
    def _mount_drive(self):
        """Mount Google Drive"""
        try:
            from google.colab import drive
            drive.mount('/content/drive')
            os.makedirs(self.drive_root, exist_ok=True)
            self.mounted = True
            print(f"üìÅ Google Drive mounted at: {self.drive_root}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to mount Google Drive: {e}")
            self.mounted = False
    
    def get_model_url(self, model_path: str) -> str:
        """Get Google Drive URL for model storage"""
        full_path = os.path.join(self.drive_root, model_path)
        return full_path
    
    def model_exists_in_drive(self, model_path: str) -> bool:
        """Check if model exists in Google Drive"""
        if not self.mounted:
            return False
        full_path = os.path.join(self.drive_root, model_path)
        return os.path.exists(full_path) and os.path.isdir(full_path)
    
    def download_model_from_drive(self, model_path: str) -> str:
        """Download model from Google Drive"""
        full_path = os.path.join(self.drive_root, model_path)
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"Model not found in Drive: {full_path}")
        return full_path
    
    def save_model_to_drive(self, local_path: str, model_path: str):
        """Save model to Google Drive"""
        if not self.mounted:
            raise RuntimeError("Google Drive not mounted")
        
        drive_path = os.path.join(self.drive_root, model_path)
        os.makedirs(os.path.dirname(drive_path), exist_ok=True)
        
        # Copy model files
        import shutil
        if os.path.exists(drive_path):
            shutil.rmtree(drive_path)
        shutil.copytree(local_path, drive_path)
        print(f"üíæ Model saved to Drive: {drive_path}")
    
    def upload_model_to_drive(self, local_path: str, model_path: str):
        """Upload model to Google Drive (alias for save_model_to_drive)"""
        return self.save_model_to_drive(local_path, model_path)

class CombinedModelLoader:
    def __init__(self):
        self.device = device
        # Pipeline 1 models
        self.phobert_model = None
        self.phobert_tokenizer = None
        # Pipeline 2 models
        self.vn_en_model = None
        self.vn_en_tokenizer = None
        self.sqlcoder_model = None
        self.sqlcoder_tokenizer = None
        
        self.cache_dir = '/content/models_cache'
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # Initialize Google Drive manager
        self.drive_manager = GoogleDriveManager()
        
        logger.info(f"üì¶ Combined Model Loader initialized on {self.device}")
        logger.info(f"üíæ Model storage location: {self.drive_manager.drive_root}")
        logger.info(f"üîó Drive mounted: {self.drive_manager.mounted}")
    
    def load_pipeline1_models(self):
        """Load PhoBERT model for Pipeline 1"""
        try:
            logger.info("Loading Pipeline 1: PhoBERT-SQL model...")
            
            model_name = "vinai/phobert-base"
            
            self.phobert_tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=self.cache_dir
            )
            
            self.phobert_model = AutoModel.from_pretrained(
                model_name,
                cache_dir=self.cache_dir,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
            ).to(self.device)
            
            logger.info(f"‚úÖ Pipeline 1 PhoBERT loaded on {self.device}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load Pipeline 1 model: {e}")
            return False
    
    def load_pipeline2_models(self):
        """Load Vietnamese‚ÜíEnglish and SQLCoder models for Pipeline 2 with Google Drive persistence"""
        try:
            # Load Vietnamese ‚Üí English model with Drive persistence
            logger.info("Loading Pipeline 2: Vietnamese ‚Üí English model...")
            
            vn_en_model_name = "Helsinki-NLP/opus-mt-vi-en"
            vn_en_drive_path = "vn_en_translator"
            
            # Display model storage URLs for debugging
            vn_en_storage_url = self.drive_manager.get_model_url(vn_en_drive_path)
            logger.info(f"üîó Vietnamese‚ÜíEnglish model storage URL: {vn_en_storage_url}")
            logger.info(f"üìÅ Model exists in Drive: {self.drive_manager.model_exists_in_drive(vn_en_drive_path)}")
            
            # Check if model exists in Drive
            if self.drive_manager.mounted and self.drive_manager.model_exists_in_drive(vn_en_drive_path):
                logger.info("üìÅ Found Vietnamese‚ÜíEnglish model in Google Drive, loading...")
                try:
                    # Load from Drive
                    local_path = self.drive_manager.download_model_from_drive(vn_en_drive_path)
                    
                    self.vn_en_tokenizer = AutoTokenizer.from_pretrained(local_path)
                    self.vn_en_model = AutoModelForSeq2SeqLM.from_pretrained(
                        local_path,
                        torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
                    ).to(self.device)
                    
                    logger.info("‚úÖ Vietnamese‚ÜíEnglish model loaded from Google Drive")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to load from Drive, downloading fresh: {e}")
                    # Fallback to fresh download
                    self._download_and_save_vn_en_model(vn_en_model_name, vn_en_drive_path)
            else:
                logger.info("üì• Downloading Vietnamese‚ÜíEnglish model for first time...")
                self._download_and_save_vn_en_model(vn_en_model_name, vn_en_drive_path)
            
            logger.info(f"‚úÖ Pipeline 2 Vietnamese‚ÜíEnglish loaded on {self.device}")
            
            # Try to load SQLCoder model with Drive persistence
            try:
                logger.info("Loading Pipeline 2: SQLCoder model...")
                
                sql_model_name = "defog/sqlcoder-7b-2"
                sql_drive_path = "sqlcoder_model"
                
                # Display model storage URLs for debugging
                sql_storage_url = self.drive_manager.get_model_url(sql_drive_path)
                logger.info(f"üîó SQLCoder model storage URL: {sql_storage_url}")
                logger.info(f"üìÅ Model exists in Drive: {self.drive_manager.model_exists_in_drive(sql_drive_path)}")
                
                # Check if SQLCoder exists in Drive
                if self.drive_manager.mounted and self.drive_manager.model_exists_in_drive(sql_drive_path):
                    logger.info("üìÅ Found SQLCoder model in Google Drive, loading...")
                    try:
                        # Load from Drive
                        local_path = self.drive_manager.download_model_from_drive(sql_drive_path)
                        
                        self.sqlcoder_tokenizer = AutoTokenizer.from_pretrained(local_path)
                        if self.sqlcoder_tokenizer.pad_token is None:
                            self.sqlcoder_tokenizer.pad_token = self.sqlcoder_tokenizer.eos_token
                        
                        self.sqlcoder_model = AutoModelForCausalLM.from_pretrained(
                            local_path,
                            torch_dtype=torch.float16,
                            device_map="auto",
                            load_in_8bit=True,
                            trust_remote_code=True
                        )
                        
                        logger.info("‚úÖ SQLCoder model loaded from Google Drive")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Failed to load SQLCoder from Drive, downloading fresh: {e}")
                        # Fallback to fresh download
                        self._download_and_save_sqlcoder_model(sql_model_name, sql_drive_path)
                else:
                    logger.info("üì• Downloading SQLCoder model for first time...")
                    self._download_and_save_sqlcoder_model(sql_model_name, sql_drive_path)
                
                logger.info(f"‚úÖ Pipeline 2 SQLCoder loaded successfully")
                return True, True  # Both models loaded
                
            except Exception as e:
                logger.error(f"‚ùå Failed to load SQLCoder: {e}")
                logger.info("üîß Pipeline 2 will use rule-based SQL generation")
                return True, False  # Translation loaded, SQL fallback
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load Pipeline 2 models: {e}")
            logger.error(f"üîç Error details: {str(e)}")
            logger.error(f"üíæ Drive mounted: {self.drive_manager.mounted}")
            logger.error(f"üìÅ Drive root: {self.drive_manager.drive_root}")
            return False, False
    
    def _download_and_save_vn_en_model(self, model_name: str, drive_path: str):
        """Download Vietnamese‚ÜíEnglish model and save to Google Drive"""
        try:
            # Download model
            self.vn_en_tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=self.cache_dir
            )
            
            self.vn_en_model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                cache_dir=self.cache_dir,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
            ).to(self.device)
            
            # Save to local cache
            local_save_path = os.path.join(self.cache_dir, "vn_en_model")
            os.makedirs(local_save_path, exist_ok=True)
            
            self.vn_en_tokenizer.save_pretrained(local_save_path)
            self.vn_en_model.save_pretrained(local_save_path)
            
            # Upload to Google Drive
            if self.drive_manager:
                logger.info("üì§ Uploading Vietnamese‚ÜíEnglish model to Google Drive...")
                self.drive_manager.upload_model_to_drive(local_save_path, drive_path)
                logger.info("‚úÖ Vietnamese‚ÜíEnglish model saved to Google Drive")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to download/save Vietnamese‚ÜíEnglish model: {e}")
            raise
    
    def _download_and_save_sqlcoder_model(self, model_name: str, drive_path: str):
        """Download SQLCoder model and save to Google Drive"""
        try:
            # Download model
            self.sqlcoder_tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=self.cache_dir
            )
            
            if self.sqlcoder_tokenizer.pad_token is None:
                self.sqlcoder_tokenizer.pad_token = self.sqlcoder_tokenizer.eos_token
            
            self.sqlcoder_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=self.cache_dir,
                torch_dtype=torch.float16,
                device_map="auto",
                load_in_8bit=True,
                trust_remote_code=True
            )
            
            # Save to local cache
            local_save_path = os.path.join(self.cache_dir, "sqlcoder_model")
            os.makedirs(local_save_path, exist_ok=True)
            
            self.sqlcoder_tokenizer.save_pretrained(local_save_path)
            self.sqlcoder_model.save_pretrained(local_save_path)
            
            # Upload to Google Drive
            if self.drive_manager:
                logger.info("üì§ Uploading SQLCoder model to Google Drive...")
                self.drive_manager.upload_model_to_drive(local_save_path, drive_path)
                logger.info("‚úÖ SQLCoder model saved to Google Drive")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to download/save SQLCoder model: {e}")
            raise

# Initialize model loader
model_loader = CombinedModelLoader()
print("üì¶ Combined Model Loader ready")

# Cell 4 - Code: Pipeline 1 Implementation
class Pipeline1:
    def __init__(self):
        self.phobert_model = None
        self.phobert_tokenizer = None
        self.logger = PipelineLogger()
        
    def load_models(self):
        """Load Pipeline 1 models"""
        success = model_loader.load_pipeline1_models()
        if success:
            self.phobert_model = model_loader.phobert_model
            self.phobert_tokenizer = model_loader.phobert_tokenizer
        return success
    
    def process(self, vietnamese_query: str, gold_sql: str = None):
        """Process Vietnamese query with comprehensive logging - NO FALLBACK LOGIC"""
        # Start metrics collection
        metrics = ExecutionMetrics()
        logger.info(f"[Pipeline1] Processing: {vietnamese_query}")
        metrics.start_measurement()
        
        try:
            # Validate model availability - FAIL FAST
            if not self.phobert_model or not self.phobert_tokenizer:
                raise RuntimeError("PhoBERT-SQL model not loaded. Cannot process query without trained model.")
            
            # Step 1: Tokenization
            tokenize_start = time.time()
            inputs = self.phobert_tokenizer(
                vietnamese_query, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=256
            )
            inputs = {k: v.to(model_loader.device) for k, v in inputs.items()}
            tokenize_time = (time.time() - tokenize_start) * 1000
            metrics.record_inference_time("tokenization", tokenize_time / 1000)
            logger.info(f"[Pipeline1] Tokenization completed in {tokenize_time:.1f}ms")
            
            # Step 2: PhoBERT Inference
            inference_start = time.time()
            with torch.no_grad():
                outputs = self.phobert_model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
            inference_time = (time.time() - inference_start) * 1000
            metrics.record_inference_time("phobert_inference", inference_time / 1000)
            logger.info(f"[Pipeline1] PhoBERT inference completed in {inference_time:.1f}ms")
            
            # Step 3: SQL Generation - MUST use trained model
            sql_gen_start = time.time()
            if not hasattr(self, 'trained_phobert_model') or not self.trained_phobert_model:
                raise RuntimeError("Trained PhoBERT-SQL model not available. Cannot generate SQL without trained model.")
            
            sql_query = self.trained_phobert_model.generate_sql(vietnamese_query, embeddings)
            sql_gen_time = (time.time() - sql_gen_start) * 1000
            metrics.record_inference_time("sql_generation", sql_gen_time / 1000)
            logger.info(f"[Pipeline1] SQL generation completed in {sql_gen_time:.1f}ms")
            
            # End metrics collection
            final_metrics = metrics.end_measurement()
            
            # Log success
            logger.info(f"[Pipeline1] Generated SQL: {sql_query}")
            
            return {
                'sql_query': sql_query,
                'execution_time': final_metrics['execution_time_ms'] / 1000,
                'pipeline': 'Pipeline1',
                'method': 'Vietnamese ‚Üí PhoBERT-SQL ‚Üí SQL',
                'evaluation_metrics': final_metrics,
                'success': True,
                'version': '1.0'
            }
            
        except Exception as e:
            error_msg = f"Pipeline1 failed: {str(e)}"
            logger.error(error_msg)
            
            # NO FALLBACK - Raise the error
            raise RuntimeError(error_msg)
    
    # REMOVED: All fallback methods deleted as per requirements

# Initialize Pipeline 1 instance
pipeline1 = Pipeline1()

# Cell 5 - Code: Pipeline 2 Implementation
class Pipeline2:
    def __init__(self):
        self.vn_en_model = None
        self.vn_en_tokenizer = None
        self.sqlcoder_model = None
        self.sqlcoder_tokenizer = None
        self.logger = PipelineLogger()
        
    def load_models(self):
        """Load Pipeline 2 models - FAIL FAST if models not available"""
        translation_success, sql_success = model_loader.load_pipeline2_models()
        
        if not translation_success:
            raise RuntimeError("Pipeline2 Vietnamese-to-English translation model failed to load")
            
        if not sql_success:
            raise RuntimeError("Pipeline2 SQLCoder model failed to load")
        
        self.vn_en_model = model_loader.vn_en_model
        self.vn_en_tokenizer = model_loader.vn_en_tokenizer
        self.sqlcoder_model = model_loader.sqlcoder_model
        self.sqlcoder_tokenizer = model_loader.sqlcoder_tokenizer
        
        return True
    
    def vietnamese_to_english(self, vietnamese_text: str):
        """Translate Vietnamese to English - FAIL FAST if model not available"""
        start_time = time.time()
        
        if not self.vn_en_model or not self.vn_en_tokenizer:
            error_msg = "Pipeline2 Vietnamese-to-English model not loaded"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        try:
            logger.info(f"[Pipeline2] Translating: {vietnamese_text}")
            
            inputs = self.vn_en_tokenizer(
                vietnamese_text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(model_loader.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.vn_en_model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=4,
                    early_stopping=True,
                    do_sample=False
                )
            
            english_text = self.vn_en_tokenizer.decode(outputs[0], skip_special_tokens=True)
            execution_time = time.time() - start_time
            
            logger.info(f"[Pipeline2] Translation: '{vietnamese_text}' ‚Üí '{english_text}'")
            return english_text, execution_time
            
        except Exception as e:
            error_msg = f"Pipeline2 Vietnamese-to-English translation failed: {str(e)}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
    
    # REMOVED: All fallback methods deleted as per requirements
    
    def english_to_sql(self, english_query: str):
        """Convert English to SQL - FAIL FAST if model not available"""
        start_time = time.time()
        
        if not self.sqlcoder_model or not self.sqlcoder_tokenizer:
            error_msg = "Pipeline2 SQLCoder model not loaded"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        try:
            logger.info(f"[Pipeline2] Generating SQL from: {english_query}")
            
            # Use SQLCoder model for SQL generation
            inputs = self.sqlcoder_tokenizer(
                english_query,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(model_loader.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.sqlcoder_model.generate(
                    **inputs,
                    max_length=256,
                    num_beams=4,
                    early_stopping=True,
                    do_sample=False
                )
            
            sql_query = self.sqlcoder_tokenizer.decode(outputs[0], skip_special_tokens=True)
            execution_time = time.time() - start_time
            
            logger.info(f"[Pipeline2] Generated SQL: {sql_query}")
            return sql_query, execution_time
            
        except Exception as e:
            error_msg = f"Pipeline2 English-to-SQL generation failed: {str(e)}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
    
    # REMOVED: All fallback methods deleted as per requirements
    
    def process(self, vietnamese_query: str, gold_sql: str = None) -> Dict[str, Any]:
        """Execute Pipeline 2: Vietnamese ‚Üí English ‚Üí SQL - FAIL FAST on errors"""
        start_time = time.time()
        
        try:
            logger.info(f"[Pipeline2] Processing: {vietnamese_query}")
            
            # Step 1: Vietnamese ‚Üí English Translation
            english_query, translate_time = self.vietnamese_to_english(vietnamese_query)
            logger.info(f"[Pipeline2] Translation completed in {translate_time:.3f}s")
            
            # Step 2: English ‚Üí SQL Generation
            sql_query, sql_gen_time = self.english_to_sql(english_query)
            logger.info(f"[Pipeline2] SQL generation completed in {sql_gen_time:.3f}s")
            
            total_time = time.time() - start_time
            
            # Create evaluation metrics
            evaluation_metrics = {
                "execution_time_ms": total_time * 1000,
                "translation_time_ms": translate_time * 1000,
                "sql_generation_time_ms": sql_gen_time * 1000,
                "ex_score": 0.0,
                "em_score": 0.0,
                "executable": True,
                "exact_match": False,
                "gpu_memory_mb": torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0,
                "inference_steps": 2,
                "error_count": 0
            }
            
            # Create detailed logs
            detailed_logs = {
                "pipeline": "Pipeline2",
                "query": vietnamese_query,
                "query_hash": str(hash(vietnamese_query))[-8:],
                "start_time": datetime.now().isoformat(),
                "end_time": datetime.now().isoformat(),
                "steps": [
                    {
                        "step": "vietnamese_to_english",
                        "duration_ms": translate_time * 1000,
                        "input": vietnamese_query,
                        "output": english_query,
                        "timestamp": datetime.now().isoformat()
                    },
                    {
                        "step": "english_to_sql",
                        "duration_ms": sql_gen_time * 1000,
                        "input": english_query,
                        "output": sql_query,
                        "timestamp": datetime.now().isoformat()
                    }
                ],
                "errors": [],
                "final_sql": sql_query,
                "english_translation": english_query,
                "gold_sql": gold_sql,
                "computation_metrics": {
                    "execution_time_ms": total_time * 1000,
                    "gpu_memory_used_mb": torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0,
                    "device": str(model_loader.device),
                    "timestamp": datetime.now().isoformat()
                },
                "execution_accuracy": {
                    "ex_score": 0.0,
                    "executable": True,
                    "error": None,
                    "result_count": 0
                },
                "exact_match": {
                    "em_score": 0.0,
                    "exact_match": False,
                    "similarity_score": 0.0
                },
                "overall_success": True
            }
            
            logger.info(f"[Pipeline2] Completed successfully: {sql_query}")
            
            return {
                'sql_query': sql_query,
                'english_translation': english_query,
                'execution_time': total_time,
                'translation_time': translate_time,
                'sql_generation_time': sql_gen_time,
                'pipeline': 'Pipeline2',
                'method': 'Vietnamese ‚Üí English ‚Üí SQL',
                'evaluation_metrics': evaluation_metrics,
                'detailed_logs': detailed_logs,
                'success': True,
                'error': None
            }
            
        except Exception as e:
            error_msg = f"Pipeline2 failed: {str(e)}"
            logger.error(error_msg)
            
            # NO FALLBACK - Raise the error
            raise RuntimeError(error_msg)

# Cell 6 - Code: Load All Models with Fail-Fast Error Handling
# Initialize both pipelines
pipeline2 = Pipeline2()

print("üöÄ Loading all models with fail-fast error handling...")

try:
    # Load Pipeline 1 (already initialized above)
    print("Loading Pipeline 1 models...")
    p1_success = pipeline1.load_models()
    print("‚úÖ Pipeline 1 models loaded successfully")
except Exception as e:
    error_msg = f"‚ùå CRITICAL ERROR: Pipeline 1 failed to load: {str(e)}"
    print(error_msg)
    print("üõë Stopping execution - cannot proceed without Pipeline 1")
    print("üîç Check model storage URLs and Drive mounting above")
    raise RuntimeError("Pipeline 1 model loading failed")

try:
    # Load Pipeline 2
    print("Loading Pipeline 2 models...")
    p2_success = pipeline2.load_models()
    print("‚úÖ Pipeline 2 models loaded successfully")
except Exception as e:
    error_msg = f"‚ùå CRITICAL ERROR: Pipeline 2 failed to load: {str(e)}"
    print(error_msg)
    print("üõë Stopping execution - cannot proceed without Pipeline 2")
    print("üîç Check model storage URLs and Drive mounting above")
    print("üíæ Verify models are properly saved to Google Drive:")
    print(f"   - Vietnamese‚ÜíEnglish: {model_loader.drive_manager.get_model_url('vn_en_translator')}")
    print(f"   - SQLCoder: {model_loader.drive_manager.get_model_url('sqlcoder_model')}")
    raise RuntimeError("Pipeline 2 model loading failed")

print("‚úÖ All models loaded successfully! Ready to start API server...")

# Cell 7 - Code: FastAPI Setup
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pydantic import BaseModel
import nest_asyncio
from pyngrok import ngrok

nest_asyncio.apply()

# Set up ngrok with your token
ngrok.set_auth_token("32BqVAspvTl3PmS23seCfxTxW93_7p3vCzKHixcdNg936rpXv")

# Create FastAPI app with version info
app = FastAPI(
    title="Vietnamese NL2SQL Pipeline API v1.0",
    description="Enhanced Vietnamese NL2SQL system with fail-fast error handling, comprehensive logging, and version control. No fallback logic - models must be loaded or requests will fail.",
    version="1.0"
)

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# Request/Response models
class QueryRequest(BaseModel):
    query: str

class EvaluationMetrics(BaseModel):
    execution_time_ms: float
    ex_score: float
    em_score: float
    executable: bool
    exact_match: bool
    gpu_memory_mb: float
    inference_steps: int
    error_count: int

class DetailedLogs(BaseModel):
    pipeline: str
    query: str
    query_hash: str
    start_time: str
    end_time: str
    steps: List[dict]
    errors: List[dict]
    final_sql: str
    gold_sql: Optional[str]
    computation_metrics: dict
    execution_accuracy: dict
    exact_match: dict
    overall_success: bool

class Pipeline1Response(BaseModel):
    sql_query: str
    execution_time: float
    pipeline: str
    method: str
    evaluation_metrics: EvaluationMetrics
    detailed_logs: DetailedLogs
    success: bool
    error: Optional[str] = None

class Pipeline2Response(BaseModel):
    sql_query: str
    english_translation: str
    execution_time: float
    translation_time: float
    sql_generation_time: float
    pipeline: str
    method: str
    sql_method: str
    evaluation_metrics: EvaluationMetrics
    detailed_logs: DetailedLogs
    success: bool
    error: Optional[str] = None

# API Endpoints with Version Info
@app.get("/")
async def root():
    return {
        "message": "Vietnamese NL2SQL Pipeline API v1.0",
        "version": "1.0",
        "status": "running",
        "device": str(model_loader.device),
        "features": {
            "fallback_logic": "removed",
            "error_handling": "fail_fast",
            "logging": "enhanced_colab_logger",
            "metrics": "comprehensive_runtime_tracking"
        },
        "pipelines": {
            "pipeline1": {
                "loaded": pipeline1.phobert_model is not None,
                "endpoint": "/pipeline1",
                "description": "Vietnamese ‚Üí PhoBERT-SQL ‚Üí SQL",
                "fallback_support": False
            },
            "pipeline2": {
                "loaded": pipeline2.vn_en_model is not None,
                "endpoint": "/pipeline2", 
                "description": "Vietnamese ‚Üí English ‚Üí SQL",
                "fallback_support": False
            }
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "1.0",
        "device": str(model_loader.device),
        "pipeline1_ready": pipeline1.phobert_model is not None,
        "pipeline2_ready": pipeline2.vn_en_model is not None,
        "fallback_logic": "disabled",
        "error_handling": "fail_fast"
    }

@app.post("/pipeline1", response_model=Pipeline1Response)
async def process_pipeline1(request: QueryRequest):
    """Pipeline 1: Vietnamese ‚Üí PhoBERT-SQL ‚Üí SQL (NO FALLBACK)"""
    try:
        logger.info(f"[API] Processing Pipeline 1 query: {request.query}")
        
        if not pipeline1.phobert_model:
            error_msg = "Pipeline 1 models not loaded - cannot process request"
            logger.error(error_msg)
            raise HTTPException(status_code=503, detail=error_msg)
        
        result = pipeline1.process(request.query)
        result["version"] = "1.0"
        
        logger.info(f"[API] Pipeline 1 completed in {result['execution_time']:.2f}s")
        return Pipeline1Response(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"Pipeline 1 processing failed: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

@app.post("/pipeline2", response_model=Pipeline2Response)
async def process_pipeline2(request: QueryRequest):
    """Pipeline 2: Vietnamese ‚Üí English ‚Üí SQL (NO FALLBACK)"""
    try:
        logger.info(f"[API] Processing Pipeline 2 query: {request.query}")
        
        if not pipeline2.vn_en_model or not pipeline2.sqlcoder_model:
            error_msg = "Pipeline 2 models not loaded - cannot process request"
            logger.error(error_msg)
            raise HTTPException(status_code=503, detail=error_msg)
        
        result = pipeline2.process(request.query)
        result["version"] = "1.0"
        
        logger.info(f"[API] Pipeline 2 completed in {result['execution_time']:.2f}s")
        return Pipeline2Response(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"Pipeline 2 processing failed: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

# Comparison endpoints for thesis evaluation
class ComparisonRequest(BaseModel):
    query: str
    method: str = "sequential"  # "sequential" or "parallel"

class ComparisonResponse(BaseModel):
    query: str
    method: str
    pipeline1_result: Pipeline1Response
    pipeline2_result: Pipeline2Response
    comparison_metrics: dict
    resource_usage: dict
    total_time: float

@app.post("/compare", response_model=ComparisonResponse)
async def compare_pipelines(request: ComparisonRequest):
    """Compare both pipelines with different execution strategies"""
    try:
        import psutil
        import asyncio
        
        start_time = time.time()
        initial_gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        initial_cpu_percent = psutil.cpu_percent(interval=None)
        
        logger.info(f"[COMPARE] Starting {request.method} comparison for: {request.query}")
        
        if request.method == "sequential":
            # Sequential execution - more accurate individual timings
            logger.info("[COMPARE] Running sequential comparison...")
            
            # Run Pipeline 1 first
            p1_start = time.time()
            p1_result = pipeline1.process(request.query)
            p1_end = time.time()
            
            # Clear GPU cache between runs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Run Pipeline 2 second
            p2_start = time.time()
            p2_result = pipeline2.process(request.query)
            p2_end = time.time()
            
            p1_response = Pipeline1Response(**p1_result)
            p2_response = Pipeline2Response(**p2_result)
            
        else:  # parallel
            # Parallel execution - real-world scenario with resource contention
            logger.info("[COMPARE] Running parallel comparison...")
            
            async def run_pipeline1():
                return pipeline1.process(request.query)
            
            async def run_pipeline2():
                return pipeline2.process(request.query)
            
            # Run both pipelines concurrently
            p1_result, p2_result = await asyncio.gather(
                asyncio.create_task(asyncio.to_thread(run_pipeline1)),
                asyncio.create_task(asyncio.to_thread(run_pipeline2))
            )
            
            p1_response = Pipeline1Response(**p1_result)
            p2_response = Pipeline2Response(**p2_result)
        
        total_time = time.time() - start_time
        final_gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        final_cpu_percent = psutil.cpu_percent(interval=None)
        
        # Calculate comparison metrics
        comparison_metrics = {
            "latency_difference_ms": abs(p1_result['execution_time'] - p2_result['execution_time']) * 1000,
            "faster_pipeline": "pipeline1" if p1_result['execution_time'] < p2_result['execution_time'] else "pipeline2",
            "speed_improvement_percent": abs(p1_result['execution_time'] - p2_result['execution_time']) / max(p1_result['execution_time'], p2_result['execution_time']) * 100,
            "sql_queries_match": p1_result['sql_query'].strip() == p2_result['sql_query'].strip(),
            "pipeline1_latency_ms": p1_result['execution_time'] * 1000,
            "pipeline2_latency_ms": p2_result['execution_time'] * 1000,
            "execution_method": request.method
        }
        
        # Resource usage metrics
        resource_usage = {
            "gpu_memory_used_mb": (final_gpu_memory - initial_gpu_memory) / (1024**2) if torch.cuda.is_available() else 0,
            "gpu_memory_peak_mb": torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0,
            "cpu_usage_change_percent": final_cpu_percent - initial_cpu_percent,
            "total_execution_time_ms": total_time * 1000,
            "device": str(model_loader.device),
            "concurrent_execution": request.method == "parallel"
        }
        
        logger.info(f"[COMPARE] {request.method.title()} comparison completed in {total_time:.2f}s")
        logger.info(f"[COMPARE] Faster: {comparison_metrics['faster_pipeline']} by {comparison_metrics['latency_difference_ms']:.1f}ms")
        
        return ComparisonResponse(
            query=request.query,
            method=request.method,
            pipeline1_result=p1_response,
            pipeline2_result=p2_response,
            comparison_metrics=comparison_metrics,
            resource_usage=resource_usage,
            total_time=total_time
        )
        
    except Exception as e:
        logger.error(f"[COMPARE] Comparison error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/benchmark")
async def benchmark_pipelines(queries: list[str] = None):
    """Run comprehensive benchmark with multiple queries"""
    if not queries:
        queries = [
            "t√¨m ƒëi·ªán tho·∫°i Samsung gi√° r·∫ª",
            "t√∫i x√°ch Apple ch·∫•t l∆∞·ª£ng cao", 
            "laptop Sony ƒë√°nh gi√° t·ªët",
            "m√°y t√≠nh b·∫£ng iPad Pro",
            "tai nghe Bluetooth kh√¥ng d√¢y"
        ]
    
    results = {
        "sequential_results": [],
        "parallel_results": [],
        "summary": {}
    }
    
    # Run sequential benchmarks
    for query in queries:
        result = await compare_pipelines(ComparisonRequest(query=query, method="sequential"))
        results["sequential_results"].append(result.dict())
    
    # Run parallel benchmarks  
    for query in queries:
        result = await compare_pipelines(ComparisonRequest(query=query, method="parallel"))
        results["parallel_results"].append(result.dict())
    
    # Calculate summary statistics
    seq_p1_times = [r["pipeline1_result"]["execution_time"] for r in results["sequential_results"]]
    seq_p2_times = [r["pipeline2_result"]["execution_time"] for r in results["sequential_results"]]
    par_p1_times = [r["pipeline1_result"]["execution_time"] for r in results["parallel_results"]]
    par_p2_times = [r["pipeline2_result"]["execution_time"] for r in results["parallel_results"]]
    
    results["summary"] = {
        "sequential": {
            "pipeline1_avg_ms": sum(seq_p1_times) / len(seq_p1_times) * 1000,
            "pipeline2_avg_ms": sum(seq_p2_times) / len(seq_p2_times) * 1000,
            "pipeline1_wins": sum(1 for r in results["sequential_results"] if r["comparison_metrics"]["faster_pipeline"] == "pipeline1"),
            "pipeline2_wins": sum(1 for r in results["sequential_results"] if r["comparison_metrics"]["faster_pipeline"] == "pipeline2")
        },
        "parallel": {
            "pipeline1_avg_ms": sum(par_p1_times) / len(par_p1_times) * 1000,
            "pipeline2_avg_ms": sum(par_p2_times) / len(par_p2_times) * 1000,
            "pipeline1_wins": sum(1 for r in results["parallel_results"] if r["comparison_metrics"]["faster_pipeline"] == "pipeline1"),
            "pipeline2_wins": sum(1 for r in results["parallel_results"] if r["comparison_metrics"]["faster_pipeline"] == "pipeline2")
        },
        "total_queries": len(queries),
        "device": str(model_loader.device)
    }
    
    return results

print("üåê FastAPI server configured")
print("üìã Available endpoints:")
print("  GET  /          - Root endpoint")
print("  GET  /health    - Health check")
print("  POST /pipeline1 - Vietnamese ‚Üí PhoBERT-SQL ‚Üí SQL")
print("  POST /pipeline2 - Vietnamese ‚Üí English ‚Üí SQL")
print("  POST /compare   - Compare both pipelines")
print("  POST /benchmark - Run comprehensive benchmark")
print("  GET  /logs      - Get session logs summary")

# Cell 8 - Code: Start Single ngrok Server with Health Check
# Add health check endpoint first
@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring server status"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "pipeline1_loaded": p1_success,
        "pipeline2_loaded": p2_success,
        "pipeline2_sql_mode": "AI Model" if not pipeline2.use_rule_based_sql else "Rule-based",
        "server_info": {
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,
            "python_version": "3.12"
        }
    }

# Start single ngrok tunnel for both pipelines
print("üöá Starting single ngrok tunnel for both pipelines...")
try:
    public_url = ngrok.connect(8000, domain="abnormally-direct-rhino.ngrok-free.app")
    print(f"üåç Combined API URL: {public_url}")
    print(f"üì° Pipeline 1 API: {public_url}/pipeline1")
    print(f"üì° Pipeline 2 API: {public_url}/pipeline2")
    
    api_url = f"{public_url}"
    print(f"\nüìã Your Combined API is available at:")
    print(f"Base URL: {api_url}")
    print(f"Health Check: {api_url}/health")
    print(f"API Docs: {api_url}/docs")
    print(f"Compare Pipelines: {api_url}/compare")
    print(f"Benchmark: {api_url}/benchmark")
    print(f"Logs Summary: {api_url}/logs")
    
    # Test the health endpoint immediately
    print(f"\nüîç Testing server health...")
    import requests
    try:
        health_response = requests.get(f"{api_url}/health", timeout=10)
        if health_response.status_code == 200:
            health_data = health_response.json()
            print(f"‚úÖ Health check passed: {health_data['status']}")
            print(f"üìä Pipeline 1: {'‚úÖ' if health_data['pipeline1_loaded'] else '‚ùå'}")
            print(f"üìä Pipeline 2: {'‚úÖ' if health_data['pipeline2_loaded'] else '‚ùå'}")
        else:
            print(f"‚ö†Ô∏è Health check failed: HTTP {health_response.status_code}")
    except Exception as health_e:
        print(f"‚ö†Ô∏è Health check error: {health_e}")
    
except Exception as e:
    print(f"‚ùå Failed to start ngrok: {e}")
    print("Please check your ngrok token and try again")

# Add logs endpoint
@app.get("/logs")
async def get_logs_summary():
    """Get comprehensive logs summary from both pipelines"""
    p1_summary = pipeline1.logger.get_session_summary()
    p2_summary = pipeline2.logger.get_session_summary()
    
    return {
        "session_summary": {
            "pipeline1": p1_summary,
            "pipeline2": p2_summary,
            "combined_stats": {
                "total_queries": p1_summary.get('total_executions', 0) + p2_summary.get('total_executions', 0),
                "overall_success_rate": (
                    (p1_summary.get('success_rate', 0) * p1_summary.get('total_executions', 0) +
                     p2_summary.get('success_rate', 0) * p2_summary.get('total_executions', 0)) /
                    max(1, p1_summary.get('total_executions', 0) + p2_summary.get('total_executions', 0))
                )
            }
        },
        "detailed_logs": {
            "pipeline1": pipeline1.logger.logs,
            "pipeline2": pipeline2.logger.logs
        }
    }

print("üåê FastAPI server configured")
print("üìã Available endpoints:")
print("  GET  /          - Root endpoint")
print("  GET  /health    - Health check")
print("  POST /pipeline1 - Vietnamese ‚Üí PhoBERT-SQL ‚Üí SQL")
print("  POST /pipeline2 - Vietnamese ‚Üí English ‚Üí SQL")
print("  POST /compare   - Compare both pipelines")
print("  POST /benchmark - Run comprehensive benchmark")
print("  GET  /logs      - Get session logs summary")

# Cell 8 - Code: Start Single ngrok Server
print("üöá Starting single ngrok tunnel for both pipelines...")
try:
    public_url = ngrok.connect(8000, domain="abnormally-direct-rhino.ngrok-free.app")
    print(f"üåç Combined API URL: {public_url}")
    print(f"üì° Pipeline 1 API: {public_url}/pipeline1")
    print(f"üì° Pipeline 2 API: {public_url}/pipeline2")
    
    api_url = f"{public_url}"
    print(f"\nüìã Your Combined API is available at:")
    print(f"Base URL: {api_url}")
    print(f"Health Check: {api_url}/health")
    print(f"API Docs: {api_url}/docs")
    print(f"Compare Pipelines: {api_url}/compare")
    print(f"Benchmark: {api_url}/benchmark")
    print(f"Logs Summary: {api_url}/logs")
    
except Exception as e:
    print(f"‚ùå Custom domain failed: {e}")
    print("üîÑ Falling back to random domain...")
    public_url = ngrok.connect(8000)
    print(f"üåç Fallback URL: {public_url}")
    api_url = f"{public_url}"

print(f"\nüöÄ Starting Combined API server on port 8000...")
print("‚ö†Ô∏è  Keep this cell running to maintain both APIs!")
print(f"üîó Configure this URL in your local system: {api_url}")

# Cell 9 - Code: Start FastAPI Server
uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
print(f"üì° Pipeline 1 Endpoint: {api_url}/pipeline1")
print(f"üì° Pipeline 2 Endpoint: {api_url}/pipeline2")

if __name__ == "__main__":
    print("MINISTRY OF EDUCATION AND TRAINING - FPT UNIVERSITY")
    print("Enhancing the User Search Experience on e-commerce Platforms Using the Deep Learning-Based Approach")
    print("Author: MSE14 Duong Dinh Tho")
    print("¬© Copyright by MSE 14 Duong Dinh Tho, 2025")
    print("Combined Pipelines Notebook - Copy this code to Google Colab and run all cells")
